# 第三章 你以为很简单的二分法

<br></br>
## 算法复杂度理论

---
### 时间复杂度

时间复杂度是面试中必问的问题。学好时间复杂度，有如下的帮助：

1. 面试官会问你的算法时间复杂度是什么
2. 当面试官说，有没有更好的方法时，你知道朝什么样的复杂度优化
3. 利用时间复杂度倒推算法是面试常用技巧。如 O(logN) 的算法几乎可以确定是二分法。

#### 什么是算法的时间复杂度

一个算法的运行时间与其所要执行的语句的数量成正比，而所要执行的语句与问题规模正相关。因此算法的时间复杂度可以表示为一个与问题规模 N 相关的多项式。

#### 面试中常见算法的时间复杂度

| 复杂度           | 可能对应的算法                                                            | 备注                           |   |   |
|------------------|---------------------------------------------------------------------------|--------------------------------|---|---|
| O(1)         | 位运算                                                                    | 常数级复杂度，一般面试中不会有 |   |   |
| O(logn)   | 二分法，倍增法，快速幂算法，辗转相除法                                    |                                |   |   |
| O(n)         | 枚举法，双指针算法，单调栈算法，KMP算法，Rabin Karp，Manacher's Algorithm | 又称作线性时间复杂度           |   |   |
| O(nlogn) | 快速排序，归并排序，堆排序                                                |                                |   |   |
| O(n^2)      | 枚举法，动态规划，Dijkstra                                                |                                |   |   |
| O(n^3)      | 枚举法，动态规划，Floyd                                                   |                                |   |   |
| O(2^n)      | 与组合有关的搜索问题                                                      |                                |   |   |
| O(n!)       | 与排列有关的搜索问题                                                      |                                |   |   |

在面试中，经常会涉及到时间复杂度的计算。当你在对于一个问题给出一种解法之后，面试官常会进一步询问，是否有更优的方法。此时就是在问你是否有时间复杂度更小的方法（有的时候也要考虑空间复杂度更小的方法），这个时候需要你对常用的数据结构操作和算法的时间复杂度有清晰的认识，从而分析出可优化的部分，给出更优的算法。

例如，给定一个已经排序的数组，现在有多次询问，每次询问一个数字是否在这个数组中，返回True or False.

* 方法1： 每次扫描一遍数组，查看是否存在。
这个方法，每次查询的时间复杂度是: O(n)。

* 方法2：由于已经有序，可以使用二分查找的方法。
这个方法，每次查询的时间复杂度是: O(logn)。

* 方法3：将数组中的数存入Hashset。
这个方法，每次查询的时间复杂度是: O(1)。

可以看到，上述的三种方法是递进的，时间复杂度越来越小。

在面试中还有很多常见常用的方法，他们的时间复杂度并不是固定的，都需要掌握其时间复杂度的分析，要能够根据算法过程自己推算出时间复杂度。


#### 用 T 函数表示法计算时间复杂度

__T 函数推导法__

我们介绍一种时间复杂度的推导方法：T函数推导法
比如二分法。二分法是每次通过 O(1) 的时间将规模为 n 的问题降低为规模为 n/2 的问题。
这里我们用 T(n) 来表示规模为 n 的问题在该算法下的时间复杂度，那么我们得出推导公式：

T(n) = T(n/2) + O(1)

我们来逐个说明一下这个公式的意义。

首先 T 代表的是 Time Complexity, n 代表的是问题规模（二分法里就是数组的大小）。
那么 T(n) 代表的就是：求处理问题规模为n的数据的时间复杂度是多少。注意这里是一个问句，不是一个答案。
T(n) 根据算法的不同可以是 O(n) 也可以是 O(nlogn) 或任何值，而 O(n) 就是 O(n)。

然后 O 代表的是时间复杂度。O(1) 就意味着，你大概用一个 if 语句，或者简单的加加减减，就可以完成。O 在这里的意思是数量级约等于。在 O 的世界里，我们只考虑最高项是什么，不考虑系数和常数项。比如：

O(100n) = O(n)

O(n^2 + n) = O(n^2)

O(2^n + n^2 + 10) = O(2^n)

__如何推导 T 函数__

我们可以使用不断展开的方法进行推导：
```
T(n) = T(n/2) + O(1)
     = T(n/4) + O(1) + O(1)
     = T(n/8) + O(1) * 3
     = T(n/16) + O(1) * 4
     ...
     = T(1) + O(1) * logn
     = O(logn)
```

---
### 空间复杂度

面试中很少问到空间复杂度，因为一般来说你看一下你开了多大的数组就是多少，比较简单。

本小节中我们快速的学习一下空间复杂度的相关知识。

#### 什么是空间复杂度

类似于时间复杂度，空间复杂度就是衡量算法运行时所占用的临时存储空间的度量。也是一个与问题规模nn有关的函数。
我们同样使用O(大写字母o)来标记。

算法所占用的空间主要有三个方面：算法代码本身占用的空间、输入输出数据占用的空间、算法运行时临时占用的空间。
其中，代码本身和输入输出数据占用的空间不是算法空间复杂度考虑的范围内，空间复杂度只考虑运行时临时占用的空间，又称为算法的额外空间（Extra space）。

临时占用的空间包括：

* 为参数列表中形参变量分配的空间
* 为函数体中局部变量分配的空间
（如果是递归函数，需要将上述两部分占用空间的和乘以递归的深度，这是堆栈空间，在下面小节中详细讲解这部分）


#### 常见算法空间复杂度

在面试中，很多时候面试官给出的问题会附带一个“不能使用多余的空间”这样的要求。很多时候这是在要求你的空间复杂度只能是O(1)O(1)的，也就是你只能开几个辅助变量，而不能开大数组。

其他常见的还有，分析一下你的算法空间复杂度，寻找空间复杂度更优的解法等。
一般来说，算法占用的时间和空间会是两个互相平衡的元素，有的时候我们牺牲空间来换取时间，有的时候我们牺牲时间来换取空间。

在面试中常见算法的空间复杂度：

* 快速排序： 最优：O(logn)，最差:O(n)
* 二分查找： O(1)
* 最短路(Dijkstra)算法： O(V)(V表示点集大小)

#### 运行时堆栈

在递归函数中，除了变量和数组所开辟的临时空间以外，还有一个空间我们需要纳入考虑，就是递归时占用的栈空间（Stack）。

递归函数需要保存当前的环境，以便在递归返回的时候能够还原之前的现场。因此递归的深度越深，所要占用的栈空间越大。当空间超出一定范围的时候就会出现程序爆栈（Stack Overflow）的情况。

很多博客文章中会写堆栈空间与递归调用的次数成正比，这个是不完全正确的，应该是与递归的深度成正比（此处只讨论单线程）。
因为递归在返回到上一层的时侯，就会将本层的空间释放，因此占用的栈空间不会比最深的一次调用所占用的空间更多。


#### 如何分析空间复杂度

__大部分的空间复杂度计算方法__

累加下面两个部分的内容即是你代码的空间复杂度：

1. 你的代码里开辟了多少新的空间（new 了多少新的内容出来）
2. 你的递归深度 * 递归函数内部的参数和局部变量所占用的空间

__以快速排序为例子__

快速排序的思路如下：
1. 选择一个基准元素，将原数组分为两部分，左边部分小于该元素，右边部分大于该元素。
2. 分别递归处理左边和右边。

* 最好情况：
每次都能恰好将数组分成左右相同长度的两部分，需要的递归深度是:lgn，每次将数组分成两部分时，我们选择不使用辅助数组，在原数组上“就地”处理，所以每层的空间是O(1)。
因此总的复杂度是:O(logn)

* 最差情况：
每次都将数组分成长度差最大的两部分，即一边只有一个元素，其余的在另外一边，最大深度为：n， 因此空间复杂度为:O(n)。

有些递归算法的空间复杂度是稳定的，不会退化，快排的递归深度与其每次选择的“基准值”有很大关系，因此存在退化的情况。

---
### 分解质因数

以 sqrt{n} 为时间复杂度的算法并不多见，最具代表性的就是分解质因数了。

题目描述
http://www.lintcode.com/problem/prime-factorization/

具体步骤
1. 记up = [sqrt{n}]，作为质因数k的上界, 初始化k=2。
2. 当k <= up 且 n不为1 时，执行步骤3，否则执行步骤4。
3. 当n被k整除时，不断整除并覆盖n，同时结果中记录k，直到n不能整出k为止。之后k自增，执行步骤2。
4. 当n不为1时，把n也加入结果当中，算法结束。

几点解释
* 不需要判定k是否为质数，如果k不为质数，且能整出n时，n早被k的因数所除。故能整除n的k必是质数。
* 为何引入up？为了优化性能。当k大于up时，k已不可能整除n，除非k是n自身。也即为何步骤4判断n是否为1，n不为1时必是比up大的质数。
* 步骤2中，也判定n是否为1，这也是为了性能，当n已为1时，可早停。

```python
def primeFactorization(n):
    result = []
    up = int(math.sqrt(n));
    
    k = 2
    while k <= up and n > 1: 
        while n % k == 0:
            n //= k
            result.append(k)
        k += 1
            
    if n > 1:
        result.append(n)
        
    return result
```

复杂度分析
最坏时间复杂度 O(sqrt_n)。当n为质数时，取到其最坏时间复杂度。
空间复杂度 O(log(n))，当n质因数很多时，需要空间大，但总不会多于O(log(n))个。

延伸
质因数分解有一种更快的算法，叫做Pollard Rho快速因数分解。该算法时间复杂度为O(n^(1/4))，其理解起来稍有难度，有兴趣的同学可以进行自学，[参考链接](https://wenku.baidu.com/view/3db5c7a6ad51f01dc381f156.html)。
 
<br></br>
## 二分法基本原理

https://1256418761.vod2.myqcloud.com/c8e8a510vodtransgzp1256418761/3d9a78527447398156687813321/v.f30.mp4

<br></br>
## 用递归的方式实现二分法

---
### 通过 Fibonacci 数列入门递归

https://1256418761.vod2.myqcloud.com/c8e8a510vodtransgzp1256418761/8cfc97ba7447398156688924811/v.f30.mp4

---
### 用递归的方式来写二分法

https://1256418761.vod2.myqcloud.com/c8e8a510vodtransgzp1256418761/cc096cfc7447398156689333827/v.f30.mp4

---
### 内存中的栈空间与堆空间

我们通常所说的内存空间，包含了两个部分：栈空间（Stack space）和堆空间（Heap space）

当一个程序在执行的时候，操作系统为了让进程可以使用一些固定的不被其他进程侵占的空间用于进行函数调用，递归等操作，会开辟一个固定大小的空间（比如 8M）给一个进程使用。这个空间不会太大，否则内存的利用率就很低。这个空间就是我们说的栈空间，Stack space。

我们通常所说的栈溢出（Stack Overflow）是指在函数调用，或者递归调用的时候，开辟了过多的内存，超过了操作系统余留的那个很小的固定空间导致的。那么哪些部分的空间会被纳入栈空间呢？栈空间主要包含如下几个部分：

1. 函数的参数与返回值
2. 函数的局部变量

```python
def f(n):
    nums = [0]*n  # 相当于Java中的new int[n]
    sum = 0
    for i in range(n):
        nums[i] = i
        sum += i
    return sum
```

根据我们的定义，参数 n，最后的函数返回值f，局部变量 sum 都很容易的可以确认是放在栈空间里的。那么主要的难点在 nums。

这里 nums 可以理解为两个部分：

1. 一个名字叫做 nums 的局部变量，他存储了指向内存空间的一个地址（Reference），这个地址也就是 4 个字节（32位地址总线的计算机，地址大小为 4 字节）

2. new 出来的，一共有 n 个位置的整数数组，int[n]。一共有 4 * n 个字节。

这里 nums 这个变量本身，是存储在栈空间的，因为他是一个局部变量。但是 nums 里存储的 n 个整数，是存储在堆空间里的，Heap space。他并不占用栈空间，并不会导致栈溢出。

在大多数的编程语言中，特别是 Java, Python 这样的语言中，万物皆对象，基本上每个变量都包含了变量自己和变量所指向的内存空间两个部分的逻辑含义。

来看这个例子：

```python
def copy(nums):
    arr = [0]*len(nums)  # 相当于Java中的new int[nums.length]
		for i in range(len(nums)):
        arr[i] = nums[i]
    return arr
		
# 用list comprehension实现同样功能
def copy(nums):
    arr = [x for x in nums]
    return arr
		
# 以下相当于Java中的main函数
if __name__ == "__main__":
    nums = [0]*10
    nums[0] = 1
    new_nums = copy(nums)
```

在 copy 这个函数中，arr 是一个局部变量，他在 copy 函数执行结束之后就会被销毁。但是里面 new 出来的新数组并不会被销毁。
这样，在 main 函数里，new_nums 里才会有被复制后的数组。所以可以发现一个特点：

>栈空间里存储的内容，会在函数执行结束的时候被撤回

简而言之可以这么区别栈空间和堆空间：

>new 出来的就放在堆空间，其他都是栈空间

---
### 什么是递归深度

什么是递归深度
递归深度就是递归函数在内存中，同时存在的最大次数。
例如下面这段求阶乘的代码：
```python
def factorial(n):
    if n == 1:
        return 1
    return factorial(n-1) * n
```

当n=100时，递归深度就是100。一般来说，我们更关心递归深度的数量级，在该阶乘函数中递归深度是O(n)O(n)，而在二分查找中，递归深度是O(log(n))。在后面的教程中，我们还会学到基于递归的快速排序、归并排序、以及平衡二叉树的遍历，这些的递归深度都是O(log(n)。注意，此处说的是递归深度，而并非时间复杂度。

__太深的递归会内存溢出__

首先，函数本身也是在内存中占空间的，主要用于存储传递的参数，以及调用代码的返回地址。

函数的调用，会在内存的栈空间中开辟新空间，来存放子函数。递归函数更是会不断占用栈空间，例如该阶乘函数，展开到最后n=1时，内存中会存在factorial(100), factorial(99), factorial(98) ... factorial(1)这些函数，它们从栈底向栈顶方向不断扩展。

当递归过深时，栈空间会被耗尽，这时就无法开辟新的函数，会报出stack overflow这样的错误。
__所以，在考虑空间复杂度时，递归函数的深度也是要考虑进去的。__

<br></br>
## 通用的二分法模板

https://1256418761.vod2.myqcloud.com/c8e8a510vodtransgzp1256418761/39706b5f7447398156687686056/v.f30.mp4

---
### 模板代码剖析

http://www.jiuzhang.com/solutions/binary-search/

常见问题

Q: 为什么要用 start + 1 < end？而不是 start < end 或者 start <= end？

A: 为了避免死循环。二分法的模板中，整个程序架构分为两个部分：

通过 while 循环，将区间范围从 n 缩小到 2 （只有 start 和 end 两个点）。
在 start 和 end 中判断是否有解。
start < end 或者 start <= end 在寻找目标最后一次出现的位置的时候，出现死循环。

Q: 为什么明明可以 start = mid + 1 偏偏要写成 start = mid?

A: 大部分时候，mid 是可以 +1 和 -1 的。在一些特殊情况下，比如寻找目标的最后一次出现的位置时，当 target 与 nums[mid] 相等的时候，是不能够使用 mid + 1 或者 mid - 1 的。因为会导致漏掉解。那么为了节省脑力，统一写成 start = mid / end = mid 并不会造成任何解的丢失，并且也不会损失效率——log(n) 和 log(n+1) 没有区别。

---
### 为什么 start < end 容易出现死循环

许多同学在写二分法的时候，都比较习惯性的写 while (start < end) 这样的循环条件。这样的写法及其容易出现死循环，导致 LintCode 上的测试“超时”（Time Limit Exceeded）。

什么情况下会出现死循环？

在做 last position of target 这种模型下的二分法时，使用 while (start < end) 就容易出现超时。

在线练习：
http://www.lintcode.com/problem/last-position-of-target/

我们来看看会超时的代码：

```python
start, end = 0, len(nums) - 1
while start <end:
    mid = start + (end - start) // 2
    if nums[mid] == target:
        start = mid
    else if nums[mid] < target:
        start = mid + 1
    else:
        end = mid - 1
```

我们发现，start 将始终是 `0`。

出现这个问题的主要原因是，mid = start + (end - start) / 2 这种写法是偏向start的。也就是说 mid 是中间偏左的位置。这样导致如果 start 和 end 已经是相邻关系，会导致 start 有可能在一轮循环之后保持不变。

或许你会说，那么我改成 mid = (start + end + 1) / 2 是否能解决问题呢？没错，确实可以解决 last position of target 的问题，但是这样之后 first position of target 就超时了。我们比较希望能够有一个理想的模板，无论是 first position of target 还是 last position of target，代码的区别尽可能的小和容易记住。

<br></br>
## 第三章课后补充内容

本节课的补充自学内容有：

* 三步翻转法及相关练习题
* 矩阵找数问题的Follow up
* 快速幂算法
* 辗转相除法

---
### 三步翻转法

https://1256418761.vod2.myqcloud.com/c8e8a510vodtransgzp1256418761/a3ec6d307447398156705089053/v.f40.mp4

---
### 二维矩阵找数问题

https://1256418761.vod2.myqcloud.com/c8e8a510vodtransgzp1256418761/08065cb97447398156690515956/v.f30.mp4

---
### 辗转相除法

算法介绍

辗转相除法， 又名欧几里德算法， 是求最大公约数的一种方法。它的具体做法是：用较大的数除以较小的数，再用除数除以出现的余数（第一余数），再用第一余数除以出现的余数（第二余数），如此反复，直到最后余数是0为止。如果是求两个数的最大公约数，那么最后的除数就是这两个数的最大公约数。

代码
```python
def gcd(big, small):
    if small != 0:
        return gcd(small, big % small)
    else:
        return big
```

LintCode 相关练习
http://www.lintcode.com/problem/greatest-common-divisor/

---
### 快速幂算法

基本原理
计算x的n次方， 即计算x^n

由公式可知： x^n = x^{n/2} * x^{n/2}

如果我们求得x^{n/2}， 则可以O(1)求出x^n, 而不需要再去循环剩下的n/2次。

以此类推，若求得x^{n/4}， 则可以O(1)求出x^{n/2}

。。。

因此一个原本O(n)的问题，我们可以用O(logn)复杂度的算法来解决。

递归版本的快速幂算法

```python
def power(x, n):
    if n == 0:
        return 1
    
    if n % 2 == 0:
        tmp = power(x, n // 2)
        return tmp * tmp
    else:
        tmp = power(x, n // 2)
        return tmp * tmp * x
```  

注意:

* 不要重复计算，在计算x^{n/2} * x^{n/2}的时候，先计算出x^{n/2}，存下来然后返回tmp*tmp 
* n为奇数的时候要记得再乘上一个xx。

非递归版本

```python
def power(x, n):
    ans = 1
    base = x
    while n > 0:
        if n % 2 == 1:
            ans *= base
        base *= base
        n = n // 2
    return ans
```

非递归版本与递归版本原理相同，计算顺序略有不同。

因为递归是从大问题进入，划分子问题然后层层返回再求解大问题。这里要从小问题开始，直接求解大问题。
你可以打印出每次循环中 base 和 ans 的值，来理清楚其中的算法思路。

递归版本和非递归版本都应该熟练掌握，虽然递归版本更容易掌握和理解，且 logn 的计算深度也不会导致 Stack Overflow。但是面试官是很有可能为了加大难度让你在用非递归的版本写一遍的。

相关练习

http://www.lintcode.com/problem/fast-power/

http://www.lintcode.com/problem/powx-n/

---
### 两个排序数组的中位数

题目描述

在两个排序数组中，求他们合并到一起之后的中位数
时间复杂度要求：O(log(n+m))，其中 n, m 分别为两个数组的长度

LintCode 练习地址：
http://www.lintcode.com/problem/median-of-two-sorted-arrays/

解法
这个题有三种做法：

1. 基于 FindKth 的算法。整体思想类似于 median of unsorted array 可以用 find kth from unsorted array 的解题思路。


2. 基于中点比较的算法。一头一尾各自丢掉一些，去掉一半的时候，整个问题的形式不变。可以推广到 median of k sorted arrays.


3. 基于二分的方法。二分 median 的值，然后再用二分法看一下两个数组里有多少个数小于这个二分出来的值。

#### 基于中点比较的算法
http://www.jiuzhang.com/solution/median-of-two-sorted-arrays/

#### 基于 FindKth 的算法
https://1256418761.vod2.myqcloud.com/c8e8a510vodtransgzp1256418761/813f82d97447398156688412296/v.f30.mp4

算法描述
1. 先将找中点问题转换为找第 k 小的问题，这里直接令k = (n + m) / 2。那么目标是在 logk = log((n+m)/2) = log(n+m) 的时间内找到A和B数组中从小到大第 k 个。
2. 比较 A 数组的第 k/2 小和 B 数组的第 k/2 小的数。谁小，就扔掉谁的前 k/2 个数。
3. 将目标寻找第 k 小修改为寻找第 (k-k/2) 小
4. 回到第 2 步继续做，直到 k == 1 或者 A 数组 B 数组里已经没有数了。

抽象的证明一下：

我们需要回顾一下 Merge Two Sorted Arrays 这道题目。算法的做法是，每一次比较两个数组中比较小的数，然后谁小，谁先被拿出来，放到最后的合并结果中。那么假设 A 和 B中 A[k/2 - 1] <= B[k/2 - 1]（反之同理）。我们会决定扔掉A[0..k/2-1]，因为这些数在 A 与 B 做简单的 Merge 的过程中，会优先于目标第 k 个数现出来。为什么？因为既然A[k/2-1] <= B[k/2-1]，那么当我们用最简单的 Merge Two Sorted Arrays 的算法一个个从A和B里拿数出来的时候，当 A[k/2 - 1] 出来的时候，B[k/2 - 1] 一定还没有被拿出来，那么此时A里出来了 k/2 个数，B里出来的数一定不够 k/2 个（因为第 k/2 个数都还没出来），所以加起来总共出来的数肯定不够k个，所以第k小的数一定还留在AB数组中。

因此我们证明了：扔掉较小的一部分的前 k/2 个数，不会扔掉要找的第 k 小的数。

#### 基于二分的算法

算法描述

1. 我们需要先确定二分的上下界限，由于两个数组 A, B 均有序，所以下界为 min(A[0], B[0])，上界为 max(A[A.length - 1], B[B.length - 1]).
2. 判断当前上下界限下的 mid(mid = (start + end) / 2) 是否为我们需要的答案；这里我们可以分别对两个数组进行二分来找到两个数组中小于等于当前 mid 的数的个数 cnt1 与 cnt2，sum = cnt1 + cnt2 即为 A 跟 B 合并后小于等于当前mid的数的个数.
3. 如果 sum < k，即中位数肯定不是 mid，应该大于 mid，更新 start 为 mid，否则更新 end 为 mid，之后再重复第二步
4. 当不满足 start + 1 < end 这个条件退出二分循环时，再分别判断一下 start 跟 end ，最终返回符合要求的那个数即可

算法详解

* 这一题如果用二分法来做，其实就是一个二分答案的过程
* 首先我们已经得到了上下界限，那么答案必定是在这个上下界限中的，需要实现的就是从这个歌上下界限中找出答案
* 我们每次取的 mid，其实就是我们每次在假设答案为 mid，二分的过程就是不断的推翻这个假设，然后再假设新的答案
* 需要满足的条件为：
  * 上面算法描述中的 sum 需要等于 k，这里的 k = (A.length + B.length) / 2. 如果 sum < k，很明显当前的 mid 偏小，需要增大，否则就说明当前的 mid 偏大，需要缩小.
* 最终在 start 与 end 相邻的时候退出循环，判断 start 跟 end 哪个符合条件即可得到最终结果
